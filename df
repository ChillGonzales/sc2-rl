[1mdiff --git a/ddpg_agent.py b/ddpg_agent.py[m
[1mindex 7d946ce..b2d0b6a 100644[m
[1m--- a/ddpg_agent.py[m
[1m+++ b/ddpg_agent.py[m
[36m@@ -22,13 +22,11 @@[m [mdef _xy_locs(mask):[m
 class DDPGAgent(Object):[m
   """A Deep Deterministic Policy Gradient implementation of an SC2 agent."""[m
 [m
[31m-  def setup(self, obs_spec, action_spec, noise_type, layer_norm=True):[m
[31m-    # Parse noise_type[m
[32m+[m[32m  def setup(self, obs_shape, action_shape, nb_actions, noise_type, gamma, tau, layer_norm=True):[m
     action_noise = None[m
     param_noise = None[m
 [m
[31m-    # we're outputting only a single action because it's continuous[m
[31m-    nb_actions = 1[m
[32m+[m[32m    # Parse noise_type[m
     for current_noise_type in noise_type.split(','):[m
         current_noise_type = current_noise_type.strip()[m
         if current_noise_type == 'none':[m
[36m@@ -46,19 +44,12 @@[m [mclass DDPGAgent(Object):[m
             raise RuntimeError('unknown noise type "{}"'.format(current_noise_type))[m
 [m
     # Configure components.[m
[31m-    self.memory = Memory(limit=int(1e6), action_shape=(1), observation_shape=obs_spec)[m
[32m+[m[32m    self.memory = Memory(limit=int(1e6), action_shape=action_shape, observation_shape=obs_shape)[m
     self.critic = Critic(layer_norm=layer_norm)[m
[31m-    self.actor = Actor(nb_actions, action_bound=action_bound, layer_norm=layer_norm, discrete=False)[m
[32m+[m[32m    self.actor = Actor(nb_actions, layer_norm=layer_norm)[m
 [m
     tf.reset_default_graph()[m
 [m
     max_action = env.action_space.high[m
[31m-    # self.agent = DDPG(actor=self.actor, critic=self.critic, memory=self.memory, env.observation_space.shape, env.action_space.shape,[m
[31m-    #     gamma=gamma, tau=tau, action_noise=action_noise, param_noise=param_noise)[m
[31m-[m
[31m-def train(self, env):[m
[31m-    # Need to refactor this[m
[31m-    training.train(env=env, param_noise=self.param_noise, action_noise=self.action_noise, [m
[31m-        actor=self.actor, critic=self.critic, memory=self.memory)[m
[31m-[m
[31m-    [m
\ No newline at end of file[m
[32m+[m[32m    self.agent = DDPG(actor=self.actor, critic=self.critic, memory=self.memory, observation_shape=env.observation_space.shape,[m[41m [m
[32m+[m[32m        action_shape=env.action_space.shape, gamma=gamma, tau=tau, action_noise=action_noise, param_noise=param_noise)[m
[1mdiff --git a/run_main.py b/run_main.py[m
[1mindex 7cfdbb4..0197c87 100644[m
[1m--- a/run_main.py[m
[1m+++ b/run_main.py[m
[36m@@ -5,6 +5,7 @@[m [mfrom pysc2.env.sc2_env import SC2Env[m
 from pysc2.env.run_loop import run_loop[m
 from ddpg_agent import DDPGAgent[m
 from collections import deque[m
[32m+[m[32mimport time[m
 import baselines.common.tf_util as U[m
 import numpy as np[m
 [m
[36m@@ -18,55 +19,47 @@[m [mdef main():[m
                 agent_interface_format=format,[m
                 visualize=True)[m
 [m
[31m-  initial_state = game.reset()[m
   agent = DDPGAgent()[m
   agent.setup(game.observation_spec, game.action_spec, noise_type=["adaptive-param", "ou"])[m
 [m
   for i in range(0, 100):[m
     # assert (np.abs(env.action_space.low) == env.action_space.high).all()  # we assume symmetric actions.[m
[31m-    obs = initial_state[m
[31m-    while not obs[0].last():[m
[32m+[m[32m    with U.single_threaded_session() as sess:[m
[32m+[m[32m      agent.initialize(sess)[m
[32m+[m[32m      sess.graph.finalize()[m
       step = 0[m
       episode = 0[m
       eval_episode_rewards_history = deque(maxlen=100)[m
       episode_rewards_history = deque(maxlen=100)[m
[31m-      with U.single_threaded_session() as sess:[m
[31m-        # Prepare everything.[m
[31m-        agent.initialize(sess)[m
[31m-        sess.graph.finalize()[m
[31m-[m
[31m-        agent.reset()[m
[31m-        obs = env.reset()[m
[31m-        done = False[m
[31m-        episode_reward = 0.[m
[31m-        episode_step = 0[m
[31m-        episodes = 0[m
[31m-        t = 0[m
[31m-[m
[31m-        epoch = 0[m
[31m-        start_time = time.time()[m
[31m-[m
[31m-        epoch_episode_rewards = [][m
[31m-        epoch_episode_steps = [][m
[31m-        epoch_episode_eval_rewards = [][m
[31m-        epoch_episode_eval_steps = [][m
[31m-        epoch_start_time = time.time()[m
[31m-        epoch_actions = [][m
[31m-        epoch_qs = [][m
[31m-        epoch_episodes = 0[m
[31m-[m
[32m+[m[32m      # Prepare everything.[m
[32m+[m[32m      agent.reset()[m
[32m+[m[32m      obs = game.reset()[0] # Only care about 1 agent right now[m
[32m+[m[32m      done = False[m
[32m+[m[32m      episode_reward = 0.[m
[32m+[m[32m      episode_step = 0[m
[32m+[m[32m      episodes = 0[m
[32m+[m[32m      t = 0[m
[32m+[m
[32m+[m[32m      epoch = 0[m
[32m+[m[32m      start_time = time.time()[m
[32m+[m
[32m+[m[32m      epoch_episode_rewards = [][m
[32m+[m[32m      epoch_episode_steps = [][m
[32m+[m[32m      epoch_episode_eval_rewards = [][m
[32m+[m[32m      epoch_episode_eval_steps = [][m
[32m+[m[32m      epoch_start_time = time.time()[m
[32m+[m[32m      epoch_actions = [][m
[32m+[m[32m      epoch_qs = [][m
[32m+[m[32m      epoch_episodes = 0[m
[32m+[m[32m      while not obs.last():[m
         # Predict next action.[m
[31m-        action, q = agent.pi(obs, apply_noise=True, compute_Q=True)[m
[31m-        assert action.shape == env.action_space.shape[m
[31m-[m
[31m-        # Execute next action.[m
[31m-        if rank == 0 and render:[m
[31m-            env.render()[m
[31m-        assert max_action.shape == action.shape[m
[31m-        new_obs, r, done, info = env.step(max_action * action)  # scale for execution in env (as far as DDPG is concerned, every action is in [-1, 1])[m
[32m+[m[32m        action, q = agent.pi(obs.observation, apply_noise=True, compute_Q=True)[m
[32m+[m[32m        # assert action.shape == game.action_spec[m
[32m+[m
[32m+[m[32m        # assert max_action.shape == action.shape[m
[32m+[m[32m        obs = env.step(max_action * action)  # scale for execution in env (as far as DDPG is concerned, every action is in [-1, 1])[m
[32m+[m[41m        [m
         t += 1[m
[31m-        if rank == 0 and render:[m
[31m-            env.render()[m
         episode_reward += r[m
         episode_step += 1[m
 [m
